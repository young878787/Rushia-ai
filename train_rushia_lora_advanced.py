import torch
from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer, 
    TrainingArguments, 
    Trainer,
    DataCollatorForLanguageModeling,
    BitsAndBytesConfig
)
from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training
from datasets import Dataset
import json
from pathlib import Path
import os
import gc
import random
import re
import warnings

# å…¨å±€ bfloat16 å„ªåŒ–è¨­ç½®
os.environ["TOKENIZERS_PARALLELISM"] = "false"
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True

# å¼·åˆ¶å…¨å±€ä½¿ç”¨ bfloat16ï¼ˆRTX 5070 æœ€ä½³é…ç½®ï¼‰
if torch.cuda.is_available() and torch.cuda.is_bf16_supported():
    torch.set_default_dtype(torch.bfloat16)
    GLOBAL_DTYPE = torch.bfloat16
    print("âœ… å…¨å±€è¨­ç½®ç‚º bfloat16ï¼ˆRTX 5070 å„ªåŒ–ï¼‰")
else:
    torch.set_default_dtype(torch.float16)
    GLOBAL_DTYPE = torch.float16
    print("âš ï¸ é™ç´šç‚º float16ï¼ˆå…¼å®¹æ¨¡å¼ï¼‰")

# é‡åŒ–è­¦å‘Šé…ç½®ï¼ˆè¨­ç‚º False ä»¥é¡¯ç¤ºæ‰€æœ‰è­¦å‘Šï¼‰
SUPPRESS_QUANTIZATION_WARNINGS = False
if SUPPRESS_QUANTIZATION_WARNINGS:
    warnings.filterwarnings("ignore", message="MatMul8bitLt")
    print("ğŸ”‡ å·²æŠ‘åˆ¶ INT8 é‡åŒ–è­¦å‘Š")
else:
    print("ğŸ”Š é¡¯ç¤ºæ‰€æœ‰é‡åŒ–è­¦å‘Šï¼ˆå¹«åŠ©ç›£æ§è¨“ç·´éç¨‹ï¼‰")

# RTX 5070 ç‰¹å®šå„ªåŒ–å»ºè­°
print("ğŸš€ RTX 5070 å„ªåŒ–å»ºè­°ï¼šINT8é‡åŒ– + gradient_checkpointing + paged_adamw_32bit + æ™ºèƒ½ç²¾åº¦é¸æ“‡")

# æ™ºèƒ½ç²¾åº¦é…ç½®æª¢æ¸¬
def detect_best_precision():
    """æª¢æ¸¬ RTX 5070 çš„æœ€ä½³ç²¾åº¦é…ç½®"""
    if torch.cuda.is_available():
        device_name = torch.cuda.get_device_name(0)
        compute_capability = torch.cuda.get_device_capability(0)
        bf16_supported = torch.cuda.is_bf16_supported()
        
        print(f"ğŸ” GPU: {device_name}")
        print(f"ğŸ” è¨ˆç®—èƒ½åŠ›: {compute_capability}")
        print(f"ğŸ” bfloat16 æ”¯æ´: {bf16_supported}")
        
        # RTX 5070 å„ªåŒ–æ±ºç­– - å¼·åˆ¶ä½¿ç”¨ bfloat16
        if bf16_supported:
            print("âœ… ä½¿ç”¨ bfloat16 â†’ int8 â†’ bfloat16 æµç¨‹")
            return {
                "use_bf16": True,
                "use_fp16": False,
                "torch_dtype": torch.bfloat16,
                "compute_dtype": torch.bfloat16  # å¼·åˆ¶ bfloat16
            }
        else:
            print("âš ï¸ é™ç´šåˆ° float16 å…¼å®¹æ¨¡å¼")
            return {
                "use_bf16": False,
                "use_fp16": True,
                "torch_dtype": torch.float16,
                "compute_dtype": torch.float16
            }
    else:
        return {
            "use_bf16": False,
            "use_fp16": False,
            "torch_dtype": torch.float32,
            "compute_dtype": torch.float32
        }

# å…¨å±€ç²¾åº¦é…ç½®
PRECISION_CONFIG = detect_best_precision()
from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training
from datasets import Dataset
import json
from pathlib import Path
import os
import gc
import random
import re

# Windows å…¼å®¹æ€§æª¢æŸ¥
import platform
IS_WINDOWS = platform.system() == "Windows"

# Flash Attention åœ¨ Windows ä¸Šä¸æ”¯æ´ï¼Œç›´æ¥è·³é
print("ğŸªŸ ä½¿ç”¨æ¨™æº– attentionï¼ˆWindows å…¼å®¹ï¼‰")

# æª¢æŸ¥è¨˜æ†¶é«”å„ªåŒ–åŠŸèƒ½
OPTIMUM_AVAILABLE = False
ACCELERATE_AVAILABLE = False

try:
    import optimum
    OPTIMUM_AVAILABLE = True 
    print("âœ… Optimum å¯ç”¨ï¼Œå°‡å•Ÿç”¨é¡å¤–è¨˜æ†¶é«”å„ªåŒ–")
except ImportError:
    print("ğŸ’¡ å»ºè­°å®‰è£ optimum: pip install optimum")

try:
    import accelerate
    ACCELERATE_AVAILABLE = True
    print("âœ… Accelerate å¯ç”¨")
except ImportError:
    print("ğŸ’¡ å»ºè­°å®‰è£ accelerate: pip install accelerate")

# RTX 5070 ç‰¹å®šå„ªåŒ–å»ºè­°
print("ï¿½ RTX 5070 å„ªåŒ–å»ºè­°ï¼šgradient_checkpointing + paged_adamw_32bit + bfloat16å„ªåŒ–å™¨ç‹€æ…‹")

class RushiaLLMTrainer:
    def __init__(self, 
                 model_path="D:/RushiaMode/models/Qwen3-8B",
                 resume_from_checkpoint=None,
                 data_category="all",
                 enable_data_augmentation=True):
        self.model_path = model_path
        self.resume_from_checkpoint = resume_from_checkpoint
        self.data_category = data_category  # "chat", "asmr", "roleplay", "all"
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.enable_data_augmentation = enable_data_augmentation
        
        # åˆå§‹åŒ–è³‡æ–™å¢å¼·å™¨
        if self.enable_data_augmentation:
            self.data_augmenter = DataAugmentation()
            print("âœ… è³‡æ–™å¢å¼·å·²å•Ÿç”¨")
        
    def load_model_and_tokenizer(self):
        """è¼‰å…¥æ¨¡å‹å’Œåˆ†è©å™¨ - bfloat16â†’8bité‡åŒ–ç‰ˆæœ¬"""
        print("è¼‰å…¥æ¨¡å‹å’Œåˆ†è©å™¨ï¼ˆbfloat16â†’8bité‡åŒ–ç‰ˆæœ¬ï¼‰...")
        
        # è¼‰å…¥åˆ†è©å™¨
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_path, 
            trust_remote_code=True,
            cache_dir=self.model_path
        )
        
        # è¨­ç½® pad_token
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        # é…ç½®8bité‡åŒ– - bfloat16 â†’ int8 â†’ bfloat16 æµç¨‹
        bnb_config = BitsAndBytesConfig(
            load_in_8bit=True,
            llm_int8_threshold=6.0,                # LLM.int8() é–¾å€¼
            llm_int8_has_fp16_weight=False,        # ä¸ä¿ç•™ fp16 æ¬Šé‡
            llm_int8_enable_fp32_cpu_offload=False, # ä¸ä½¿ç”¨ CPU offload
            # å¼·åˆ¶ä½¿ç”¨ bfloat16 ä½œç‚ºè¨ˆç®—ç²¾åº¦ï¼ˆRTX 5070 å„ªåŒ–ï¼‰
            # æ³¨æ„ï¼šbitsandbytes å¯èƒ½ä»æœƒåœ¨å…§éƒ¨è½‰æ›ï¼Œä½†æˆ‘å€‘æŒ‡å®š bfloat16
        )
        
        # è¼‰å…¥æ¨¡å‹ - RTX 5070 æ™ºèƒ½é…ç½®ï¼ˆQwen3-8B + 8bité‡åŒ–ï¼‰
        model_kwargs = {
            "trust_remote_code": True,
            "quantization_config": bnb_config,
            "torch_dtype": PRECISION_CONFIG["torch_dtype"],  # æ™ºèƒ½ç²¾åº¦é¸æ“‡
            "device_map": "auto",
            "low_cpu_mem_usage": True,
            "max_memory": {0: "10GB"}  # 8bité‡åŒ–è¨˜æ†¶é«”é…ç½®
        }
        
        print("ğŸš€ RTX 5070 ç’°å¢ƒï¼šbfloat16 â†’ INT8 â†’ bfloat16 å®Œæ•´æµç¨‹")
        
        # è¨“ç·´æ™‚çš„é€šç”¨å„ªåŒ–
        model_kwargs["use_cache"] = False  # è¨“ç·´æ™‚ç¦ç”¨KV cacheç¯€çœé¡¯å­˜
        
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_path,
            **model_kwargs
        )
        
        # æº–å‚™æ¨¡å‹é€²è¡Œbfloat16â†’8bitè¨“ç·´ - é—œéµæ­¥é©Ÿï¼
        self.model = prepare_model_for_kbit_training(
            self.model, 
            use_gradient_checkpointing=True  # æ˜ç¢ºå•Ÿç”¨æ¢¯åº¦æª¢æŸ¥é»
        )
        
        # ç¢ºä¿æ¨¡å‹ä½¿ç”¨æ­£ç¢ºçš„ç²¾åº¦
        print(f"ğŸ¯ æ¨¡å‹æ¬Šé‡ç²¾åº¦: {next(self.model.parameters()).dtype}")
        
        # å•Ÿç”¨æ¢¯åº¦æª¢æŸ¥é»ä¾†ç¯€çœé¡¯å­˜
        if hasattr(self.model, 'gradient_checkpointing_enable'):
            self.model.gradient_checkpointing_enable()
            print("âœ… æ¢¯åº¦æª¢æŸ¥é»å·²å•Ÿç”¨")
        
        # æ¨™æº–attentioné…ç½®å®Œæˆ
        print("âœ… bfloat16 â†’ INT8 â†’ bfloat16 æµç¨‹é…ç½®å®Œæˆ")
        
        print("æ¨¡å‹è¼‰å…¥å®Œæˆï¼ˆbfloat16 â†’ 8bité‡åŒ– â†’ bfloat16 è¼¸å‡ºï¼‰ï¼")
        
    def setup_lora(self, existing_adapter_path=None):
        """è¨­ç½®LoRAé…ç½® - 8bité‡åŒ–ç‰ˆæœ¬"""
        if existing_adapter_path and os.path.exists(existing_adapter_path):
            print(f"è¼‰å…¥ç¾æœ‰LoRAé©é…å™¨: {existing_adapter_path}")
            from peft import PeftModel
            self.model = PeftModel.from_pretrained(self.model, existing_adapter_path)
        else:
            print("å‰µå»ºæ–°çš„LoRAé©é…å™¨")
            lora_config = LoraConfig(
                task_type=TaskType.CAUSAL_LM,
                inference_mode=False,
                r=64,  # å®˜æ–¹QLoRAè«–æ–‡æ¨è–¦64
                lora_alpha=16,  # alphaé€šå¸¸è¨­ç‚ºr/4
                lora_dropout=0.1,  # å®˜æ–¹æ¨è–¦0.1
                target_modules=["q_proj", "v_proj", "k_proj", "o_proj", 
                              "gate_proj", "up_proj", "down_proj"],
                bias="none",
                modules_to_save=None,
                use_rslora=False,  # å¯é¸ï¼šå•Ÿç”¨RSLoRA
                use_dora=False,    # å¯é¸ï¼šå•Ÿç”¨DoRA
            )
            
            self.model = get_peft_model(self.model, lora_config)
        
        # å•Ÿç”¨è¨“ç·´æ¨¡å¼
        self.model.train()
        self.model.print_trainable_parameters()
        
    def load_training_data(self, data_file=None, max_samples=1000, shuffle=True):
        """è¼‰å…¥è¨“ç·´æ•¸æ“š - æ”¯æŒåˆ†é¡åˆ¥æ•¸æ“šæ–‡ä»¶ï¼Œè³‡æ–™æ··æ´—å’Œå¢å¼·"""
        print(f"è¼‰å…¥è¨“ç·´æ•¸æ“š - é¡åˆ¥: {self.data_category}")
        
        # æ ¹æ“šé¡åˆ¥è‡ªå‹•é¸æ“‡å°æ‡‰çš„æ•¸æ“šæ–‡ä»¶
        if data_file is None:
            if self.data_category == "asmr":
                data_file = "D:/RushiaMode/training_data/by_category/rushia_ASMR_training.jsonl"
            elif self.data_category == "chat":
                data_file = "D:/RushiaMode/training_data/by_category/rushia_é›œè«‡_training.jsonl"
            elif self.data_category == "roleplay":
                data_file = "D:/RushiaMode/training_data/by_category/rushia_roleplay_training.jsonl"
            else:  # all or other categories
                data_file = "D:/RushiaMode/training_data/rushia_mixed_training.jsonl"
        
        print(f"ä½¿ç”¨æ•¸æ“šæ–‡ä»¶: {data_file}")
        
        # æª¢æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(data_file):
            print(f"éŒ¯èª¤: æ•¸æ“šæ–‡ä»¶ä¸å­˜åœ¨ - {data_file}")
            print("è«‹å…ˆé‹è¡Œ prepare_training_data_enhanced.py ç”Ÿæˆè¨“ç·´æ•¸æ“š")
            return []
        
        conversations = []
        with open(data_file, 'r', encoding='utf-8') as f:
            for line_count, line in enumerate(f):
                if line_count >= max_samples:
                    break
                data = json.loads(line)
                conversations.append(data)
        
        print(f"è¼‰å…¥æ•¸æ“šé‡: {len(conversations)} (é™åˆ¶: {max_samples})")
        
        # è³‡æ–™æ··æ´—
        if shuffle:
            random.shuffle(conversations)
            print("âœ… è³‡æ–™å·²æ··æ´—")
        
        # è½‰æ›ç‚ºè¨“ç·´æ ¼å¼ä¸¦æ‡‰ç”¨è³‡æ–™å¢å¼·
        texts = []
        augmented_count = 0
        
        for conv in conversations:
            text = ""
            for turn in conv["conversations"]:
                if turn["from"] == "human":
                    user_text = turn['value']
                    # å°ç”¨æˆ¶è¼¸å…¥é€²è¡Œå¢å¼·
                    if self.enable_data_augmentation:
                        user_text = self.data_augmenter.augment_text(user_text)
                    text += f"ç”¨æˆ¶: {user_text}\n"
                else:
                    assistant_text = turn['value']
                    # å°åŠ©ç†å›æ‡‰é€²è¡Œå¢å¼·
                    if self.enable_data_augmentation:
                        original_text = assistant_text
                        assistant_text = self.data_augmenter.augment_text(assistant_text)
                        if assistant_text != original_text:
                            augmented_count += 1
                    text += f"ã‚‹ã—ã‚: {assistant_text}\n"
            text += self.tokenizer.eos_token
            texts.append(text)
        
        if self.enable_data_augmentation:
            print(f"âœ… è³‡æ–™å¢å¼·å®Œæˆï¼Œå¢å¼·äº† {augmented_count} æ¢å›æ‡‰")
        
        return texts    
    def tokenize_data(self, texts, max_length=512):
        """åˆ†è©è™•ç† - æ”¯æ´å‹•æ…‹ padding"""
        def tokenize_function(examples):
            tokenized = self.tokenizer(
                examples["text"],
                truncation=True,
                padding=False,  # é—œéµï¼šä¸åœ¨é€™è£¡ paddingï¼Œè®“ DataCollator è™•ç†
                max_length=max_length,
                return_tensors=None
            )
            # å°æ–¼èªè¨€æ¨¡å‹ï¼Œlabelså°±æ˜¯input_ids
            tokenized["labels"] = tokenized["input_ids"].copy()
            return tokenized
        
        dataset = Dataset.from_dict({"text": texts})
        
        # å•Ÿç”¨è³‡æ–™æ··æ´—
        dataset = dataset.shuffle(seed=42)
        print("âœ… Dataset å±¤ç´šæ··æ´—å·²å•Ÿç”¨")
        
        tokenized_dataset = dataset.map(
            tokenize_function, 
            batched=True,
            remove_columns=["text"],
            batch_size=50  # æ‰¹è™•ç†å¤§å°
        )
        
        print(f"âœ… åˆ†è©å®Œæˆï¼Œåºåˆ—æœ€å¤§é•·åº¦: {max_length}")
        return tokenized_dataset
    
    def train(self, output_dir=None, epochs=5, max_samples=2000, max_seq_length=512):
        """é–‹å§‹è¨“ç·´ - INT8é‡åŒ–å„ªåŒ–ç‰ˆæœ¬ï¼Œæ”¯æ´å‹•æ…‹paddingå’Œè³‡æ–™æ··æ´„"""
        if output_dir is None:
            output_dir = f"D:/RushiaMode/models/rushia-qwen3-8b-lora-{self.data_category}-8bit"
        
        print(f"ğŸš€ é–‹å§‹INT8é‡åŒ–è¨“ç·´ - é¡åˆ¥: {self.data_category}")
        print(f"ğŸ“ è¼¸å‡ºç›®éŒ„: {output_dir}")
        print(f"ğŸ“ æœ€å¤§åºåˆ—é•·åº¦: {max_seq_length}")
        print(f"ğŸ¯ ç²¾åº¦é…ç½®: {PRECISION_CONFIG}")
        
        # GPU å’Œè¨˜æ†¶é«”è¨ºæ–·ä¿¡æ¯
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name(0)
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            print(f"ğŸ® GPU: {gpu_name}")
            print(f"ğŸ’¾ ç¸½è¨˜æ†¶é«”: {gpu_memory:.1f}GB")
            print(f"ğŸ’¾ ç•¶å‰ä½¿ç”¨: {torch.cuda.memory_allocated(0) / 1024**3:.1f}GB")
        
        # è¼‰å…¥æ•¸æ“šï¼ˆé™åˆ¶æ¨£æœ¬æ•¸ï¼Œå•Ÿç”¨æ··æ´—ï¼‰
        texts = self.load_training_data(max_samples=max_samples, shuffle=True)
        if not texts:
            print("âŒ æ²’æœ‰è¼‰å…¥åˆ°è¨“ç·´æ•¸æ“šï¼Œé€€å‡ºè¨“ç·´")
            return None
            
        dataset = self.tokenize_data(texts, max_length=max_seq_length)
        
        # è¨“ç·´åƒæ•¸ï¼ˆWindows å„ªåŒ–é…ç½®ï¼‰
        training_args_dict = {
            "output_dir": output_dir,
            "overwrite_output_dir": False,
            "num_train_epochs": epochs,
            "per_device_train_batch_size": 2,  
            "gradient_accumulation_steps": 8,  
            "warmup_steps": 100,               # æ›´å¤šwarmupæ­¥æ•¸
            "logging_steps": 10,
            "save_steps": 500,
            "save_total_limit": 2,
            "learning_rate": 1e-4,
            "bf16": PRECISION_CONFIG["use_bf16"],      # æ™ºèƒ½ bfloat16 é¸æ“‡
            "fp16": PRECISION_CONFIG["use_fp16"],      # æ™ºèƒ½ float16 é¸æ“‡
            "remove_unused_columns": False,
            "dataloader_pin_memory": False,
            "gradient_checkpointing": True,    # å¼·åˆ¶å•Ÿç”¨æ¢¯åº¦æª¢æŸ¥é»
            "dataloader_num_workers": 4,       # RTX 5070 å¯ä»¥ä½¿ç”¨æ›´å¤š workers
            "resume_from_checkpoint": self.resume_from_checkpoint,
            "max_grad_norm": 0.3,             # å®˜æ–¹æ¨è–¦0.3
            "logging_dir": f"{output_dir}/logs",
            "report_to": [],
            "push_to_hub": False,
            "label_names": ["labels"],
            "optim": "paged_adamw_32bit",     # Paged optimizer for memory efficiency
            "lr_scheduler_type": "cosine",    # å®˜æ–¹æ¨è–¦cosineèª¿åº¦å™¨
            # è³‡æ–™æ··æ´—è¨­ç½®
            "dataloader_drop_last": False,    # ä¸ä¸Ÿæ£„æœ€å¾Œä¸€å€‹ä¸å®Œæ•´çš„batch
            "group_by_length": False,         # é—œé–‰æŒ‰é•·åº¦åˆ†çµ„ï¼Œè®“å‹•æ…‹paddingç™¼æ®ä½œç”¨
            # è¨˜æ†¶é«”å„ªåŒ–ç›¸é—œè¨­ç½®
            "ddp_find_unused_parameters": False,  # é¿å…DDPå•é¡Œ
            "save_safetensors": True,             # ä½¿ç”¨æ›´å®‰å…¨çš„ä¿å­˜æ ¼å¼
            # é€²éšè¨˜æ†¶é«”ç®¡ç†
            "max_steps": -1,                      # è®“epochæ§åˆ¶è¨“ç·´
            "eval_steps": None,                   # ä¸é€²è¡Œè©•ä¼°ç¯€çœæ™‚é–“
            "prediction_loss_only": True,        # åªè¨ˆç®—lossç¯€çœè¨ˆç®—
        }
        
        # åªæœ‰åœ¨å¤šç·šç¨‹æ™‚æ‰æ·»åŠ  prefetch_factor
        if training_args_dict["dataloader_num_workers"] > 0:
            training_args_dict["dataloader_prefetch_factor"] = 2
        
        training_args = TrainingArguments(**training_args_dict)
        
        # ä½¿ç”¨å‹•æ…‹è³‡æ–™æ•´ç†å™¨
        data_collator = DynamicDataCollator(
            tokenizer=self.tokenizer,
            mlm=False,
            max_length=max_seq_length
        )
        print("âœ… å‹•æ…‹ padding è³‡æ–™æ•´ç†å™¨å·²å•Ÿç”¨")
        
        # è‡ªå®šç¾©è¨“ç·´å™¨æ”¯æ´æ¯å€‹ epoch å‰æ··æ´—ï¼ˆç°¡åŒ–ç‰ˆæœ¬ï¼‰
        class ShufflingTrainer(Trainer):
            def __init__(self, **kwargs):
                super().__init__(**kwargs)
                self.epoch_count = 0
                
            def get_train_dataloader(self):
                """æ¯å€‹ epoch å‰é‡æ–°æ··æ´—è³‡æ–™"""
                if self.epoch_count > 0:  # ç¬¬ä¸€å€‹epochå·²ç¶“åœ¨dataset.shuffle()ä¸­æ··æ´—éäº†
                    print(f"ğŸ”€ Epoch {self.epoch_count + 1}: é‡æ–°æ··æ´—è¨“ç·´è³‡æ–™")
                    self.train_dataset = self.train_dataset.shuffle(seed=42 + self.epoch_count)
                self.epoch_count += 1
                return super().get_train_dataloader()
            
            # ç§»é™¤è¤‡é›œçš„ training_step è¦†è“‹ï¼Œä½¿ç”¨åŸç”Ÿå¯¦ç¾
            # Transformers æœƒè‡ªå‹•è™•ç† autocast å’Œç²¾åº¦ç®¡ç†
        
        # è¨“ç·´å™¨
        trainer = ShufflingTrainer(
            model=self.model,
            args=training_args,
            train_dataset=dataset,
            data_collator=data_collator,
        )
        
        # é–‹å§‹è¨“ç·´
        import time
        start_time = time.time()
        print("ğŸš€ é–‹å§‹è¨“ç·´...")
        print(f"â° é–‹å§‹æ™‚é–“: {time.strftime('%Y-%m-%d %H:%M:%S')}")
        print("âœ… æ¯å€‹ epoch å‰è‡ªå‹•æ··æ´—å·²å•Ÿç”¨")
        print("âœ… å‹•æ…‹ padding å·²å•Ÿç”¨ï¼ˆæ ¹æ“š batch æœ€å¤§é•·åº¦ï¼‰")
        print("âš ï¸ é‡åŒ–è­¦å‘Šé¡¯ç¤ºå·²å•Ÿç”¨ï¼ˆç›£æ§ç²¾åº¦è½‰æ›ï¼‰")
        if self.enable_data_augmentation:
            print("âœ… è³‡æ–™å¢å¼·å·²å•Ÿç”¨")
        
        # è¨“ç·´å‰è¨˜æ†¶é«”ç‹€æ…‹
        if torch.cuda.is_available():
            memory_before = torch.cuda.memory_allocated(0) / 1024**3
            print(f"ğŸ’¾ è¨“ç·´å‰è¨˜æ†¶é«”ä½¿ç”¨: {memory_before:.1f}GB")
            torch.cuda.empty_cache()
            gc.collect()
            memory_after_cleanup = torch.cuda.memory_allocated(0) / 1024**3
            print(f"ğŸ’¾ æ¸…ç†å¾Œè¨˜æ†¶é«”ä½¿ç”¨: {memory_after_cleanup:.1f}GB")
            print("ğŸ§¹ GPUè¨˜æ†¶é«”å·²æ¸…ç†")
        
        trainer.train(resume_from_checkpoint=self.resume_from_checkpoint)
        
        # è¨“ç·´å®Œæˆçµ±è¨ˆ
        end_time = time.time()
        training_duration = end_time - start_time
        hours = int(training_duration // 3600)
        minutes = int((training_duration % 3600) // 60)
        seconds = int(training_duration % 60)
        
        print(f"â±ï¸ è¨“ç·´è€—æ™‚: {hours:02d}:{minutes:02d}:{seconds:02d}")
        print(f"â° çµæŸæ™‚é–“: {time.strftime('%Y-%m-%d %H:%M:%S')}")
        
        # æœ€çµ‚è¨˜æ†¶é«”ç‹€æ…‹
        if torch.cuda.is_available():
            final_memory = torch.cuda.memory_allocated(0) / 1024**3
            max_memory = torch.cuda.max_memory_allocated(0) / 1024**3
            print(f"ğŸ’¾ æœ€çµ‚è¨˜æ†¶é«”ä½¿ç”¨: {final_memory:.1f}GB")
            print(f"ğŸ’¾ å³°å€¼è¨˜æ†¶é«”ä½¿ç”¨: {max_memory:.1f}GB")
        
        # ä¿å­˜æœ€çµ‚æ¨¡å‹
        print("ğŸ’¾ ä¿å­˜æ¨¡å‹...")
        trainer.save_model()
        self.tokenizer.save_pretrained(output_dir)
        
        print(f"âœ… è¨“ç·´å®Œæˆï¼æ¨¡å‹å·²ä¿å­˜åˆ°: {output_dir}")
        return output_dir

class DataAugmentation:
    """è³‡æ–™å¢å¼·é¡åˆ¥"""
    
    def __init__(self):
        # åŒç¾©è©æ›¿æ›å­—å…¸ï¼ˆASMRå’ŒèŠå¤©ç›¸é—œï¼‰
        self.synonyms = {
            "å¥½çš„": ["å¥½~", "å¥½å‘¢", "å¥½å‘€", "å—¯å—¯", "ok", "OK"],
            "è²éŸ³": ["éŸ³è‰²", "å—“éŸ³", "voice"],
            "å¯æ„›": ["kawaii", "å¡å“‡ä¼Š", "è¶…èŒ", "å¥½èŒ"],
            "èˆ’æœ": ["æ”¾é¬†", "ç™‚ç™’", "comfort", "relax"],
            "ç¡è¦º": ["ç¡çœ ", "ä¼‘æ¯", "sleep", "å…¥çœ "],
            "è€³æœµ": ["ears", "è€³æœµæœµ", "å°è€³æœµ"],
            "è¼•æŸ”": ["æº«æŸ”", "gentle", "soft", "è¼•è»Ÿ"],
            "å®‰éœ": ["quiet", "å¯§éœ", "peaceful"],
            "è¬è¬": ["æ„Ÿè¬", "thank you", "3Q", "thx"],
            "æ™šå®‰": ["good night", "ãŠã‚„ã™ã¿", "ç¡å€‹å¥½è¦º"],
            "å‘¢": ["å‘€", "å“¦", "å–”", "~"],
            "å¾ˆ": ["éå¸¸", "è¶…", "å¥½", "ç‰¹åˆ¥"],
            "çœŸçš„": ["ç¢ºå¯¦", "çš„ç¢º", "really", "çœŸæ˜¯"],
            "ä¸€èµ·": ["together", "å…±åŒ", "ä¸€åŒ"],
            "å–œæ­¡": ["æ„›", "like", "love", "é¾æ„›"],
        }
        
        # è¡¨æƒ…ç¬¦è™Ÿå¢å¼·
        self.emoticons = ["~", "â™¡", "â™ª", "â˜†", "â—‡", "â—‹", "â–³", "â–½", "â™¬", "â™«"]
        
    def augment_text(self, text, augment_prob=0.3):
        """å°æ–‡æœ¬é€²è¡Œå¢å¼·"""
        if random.random() > augment_prob:
            return text
            
        # 1. åŒç¾©è©æ›¿æ›
        text = self._synonym_replacement(text)
        
        # 2. éš¨æ©Ÿæ’å…¥è¡¨æƒ…ç¬¦è™Ÿ
        text = self._add_emoticons(text)
        
        # 3. æ¨™é»ç¬¦è™Ÿè®ŠåŒ–
        text = self._punctuation_variation(text)
        
        return text
    
    def _synonym_replacement(self, text):
        """åŒç¾©è©æ›¿æ›"""
        for word, synonyms in self.synonyms.items():
            if word in text and random.random() < 0.3:
                replacement = random.choice(synonyms)
                text = text.replace(word, replacement, 1)  # åªæ›¿æ›ç¬¬ä¸€å€‹
        return text
    
    def _add_emoticons(self, text):
        """æ·»åŠ è¡¨æƒ…ç¬¦è™Ÿ"""
        if random.random() < 0.2:  # 20% æ©Ÿç‡æ·»åŠ è¡¨æƒ…
            emoticon = random.choice(self.emoticons)
            if text.endswith(('ã€‚', 'ï¼', 'ï¼Ÿ', '~', 'â™¡')):
                text = text + emoticon
            else:
                text = text + emoticon
        return text
    
    def _punctuation_variation(self, text):
        """æ¨™é»ç¬¦è™Ÿè®ŠåŒ–"""
        # éš¨æ©Ÿå°‡å¥è™Ÿæ”¹ç‚ºå…¶ä»–ç¬¦è™Ÿ
        if random.random() < 0.2:
            text = text.replace('ã€‚', random.choice(['~', 'â™¡', 'å‘¢~', 'å“¦~']))
        return text

class DynamicDataCollator(DataCollatorForLanguageModeling):
    """å‹•æ…‹ padding çš„è³‡æ–™æ•´ç†å™¨ - bfloat16 å„ªåŒ–ç‰ˆæœ¬"""
    
    def __init__(self, tokenizer, mlm=False, max_length=None):
        super().__init__(tokenizer, mlm)
        self.max_length = max_length
        
    def __call__(self, examples):
        # æ‰¾å‡º batch ä¸­æœ€é•·çš„åºåˆ—
        if self.max_length:
            max_len = min(max([len(ex['input_ids']) for ex in examples]), self.max_length)
        else:
            max_len = max([len(ex['input_ids']) for ex in examples])
        
        # å‹•æ…‹ padding åˆ° batch æœ€å¤§é•·åº¦
        batch = []
        for example in examples:
            input_ids = example['input_ids'][:max_len]  # æˆªæ–·
            attention_mask = example['attention_mask'][:max_len]
            labels = example['labels'][:max_len]
            
            # Padding åˆ° batch æœ€å¤§é•·åº¦
            pad_length = max_len - len(input_ids)
            if pad_length > 0:
                input_ids.extend([self.tokenizer.pad_token_id] * pad_length)
                attention_mask.extend([0] * pad_length)
                labels.extend([-100] * pad_length)
            
            batch.append({
                'input_ids': input_ids,
                'attention_mask': attention_mask,
                'labels': labels
            })
        
        # è½‰æ›ç‚º tensorï¼Œä¸¦ç¢ºä¿ä½¿ç”¨å…¨å±€ DTYPE
        result = {
            'input_ids': torch.tensor([ex['input_ids'] for ex in batch], dtype=torch.long),
            'attention_mask': torch.tensor([ex['attention_mask'] for ex in batch], dtype=torch.long),
            'labels': torch.tensor([ex['labels'] for ex in batch], dtype=torch.long)
        }
        
        # å°æ–¼éœ€è¦æµ®é»é‹ç®—çš„å¼µé‡ï¼Œç¢ºä¿ä½¿ç”¨ bfloat16
        # input_ids, attention_mask, labels ä¿æŒ long é¡å‹
        # å…¶ä»–è¨ˆç®—æœƒè‡ªå‹•ä½¿ç”¨å…¨å±€è¨­å®šçš„ bfloat16
        
        return result

def check_gpu_memory():
    """æª¢æŸ¥GPUé¡¯å­˜æƒ…æ³"""
    if torch.cuda.is_available():
        for i in range(torch.cuda.device_count()):
            props = torch.cuda.get_device_properties(i)
            total_memory = props.total_memory / (1024**3)  # GB
            print(f"GPU {i}: {props.name}")
            print(f"  ç¸½é¡¯å­˜: {total_memory:.1f} GB")
            
            if torch.cuda.is_initialized():
                allocated = torch.cuda.memory_allocated(i) / (1024**3)
                cached = torch.cuda.memory_reserved(i) / (1024**3)
                print(f"  å·²åˆ†é…: {allocated:.1f} GB")
                print(f"  å·²ç·©å­˜: {cached:.1f} GB")
                print(f"  å¯ç”¨: {total_memory - cached:.1f} GB")
            
            return total_memory
    else:
        print("æœªæª¢æ¸¬åˆ°CUDA GPU")
        return 0

def optimize_memory_settings():
    """å„ªåŒ–è¨˜æ†¶é«”è¨­ç½®"""
    if torch.cuda.is_available():
        # å•Ÿç”¨è¨˜æ†¶é«”åˆ†ç‰‡
        torch.cuda.empty_cache()
        
        # è¨­ç½®è¨˜æ†¶é«”åˆ†é…ç­–ç•¥
        if hasattr(torch.cuda, 'set_memory_fraction'):
            torch.cuda.set_memory_fraction(0.9)  # ä½¿ç”¨90%é¡¯å­˜
            
        # å•Ÿç”¨cudnn benchmark
        torch.backends.cudnn.deterministic = False
        torch.backends.cudnn.benchmark = True  # å„ªåŒ– CUDNN
        torch.backends.cuda.matmul.allow_tf32 = True  # å•Ÿç”¨ TF32ï¼ˆRTX 5070 æ”¯æ´ï¼‰
        
        print("âœ… è¨˜æ†¶é«”å„ªåŒ–è¨­ç½®å®Œæˆ")
        print("ï¿½ ä½¿ç”¨æ¨™æº–attention + gradient_checkpointing + paged_adamw_32bit å„ªåŒ–é¡¯å­˜")
            
        return True
    return False

def optimize_for_rtx5070():
    """RTX 5070 å°ˆç”¨å„ªåŒ–è¨­ç½®"""
    print("ï¿½ æ‡‰ç”¨ RTX 5070 å°ˆç”¨å„ªåŒ–...")
    
    # RTX 5070 è¨˜æ†¶é«”ç®¡ç†å„ªåŒ–
    if torch.cuda.is_available():
        # æ¸…ç† GPU è¨˜æ†¶é«”
        torch.cuda.empty_cache()
        
        # è¨­ç½®è¨˜æ†¶é«”åˆ†é…ç­–ç•¥ï¼ˆRTX 5070 12GB å„ªåŒ–ï¼‰
        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:256,expandable_segments:True'
            
        # Windows ä¸‹é—œé–‰ä¸€äº›å¯èƒ½æœ‰å•é¡Œçš„å„ªåŒ–
        torch.backends.cudnn.benchmark = False  # Windowsä¸‹å¯èƒ½ä¸ç©©å®š
        torch.backends.cudnn.deterministic = True
        
        # è¨­ç½®æ›´ä¿å®ˆçš„ç·šç¨‹æ•¸
        torch.set_num_threads(min(4, torch.get_num_threads()))
        
        print("âœ… Windows å„ªåŒ–è¨­ç½®å®Œæˆ")
    else:
        print("ğŸ§ Linux/Unix ç’°å¢ƒï¼šå•Ÿç”¨é«˜æ€§èƒ½è¨­ç½®")
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.deterministic = False

def train_asmr_lora(epochs=3, max_samples=1000):
    """å°ˆé–€è¨“ç·´ASMR LoRAæ¨¡å‹"""
    print("ğŸ§ é–‹å§‹è¨“ç·´ASMRå°ˆç”¨LoRAæ¨¡å‹ (4bité‡åŒ–)")
    print("="*60)
    
    trainer = RushiaLLMTrainer(data_category="asmr")
    
    try:
        # è¼‰å…¥æ¨¡å‹ï¼ˆ4bité‡åŒ–ï¼‰
        trainer.load_model_and_tokenizer()
        
        # è¨­ç½®LoRA
        trainer.setup_lora()
        
        # é–‹å§‹è¨“ç·´
        model_path = trainer.train(epochs=epochs, max_samples=max_samples)
        
        if model_path:
            print(f"\nğŸ‰ ASMR LoRAæ¨¡å‹è¨“ç·´å®Œæˆï¼")
            print(f"ğŸ“ æ¨¡å‹ä¿å­˜åœ¨: {model_path}")
            return model_path
        else:
            print("âŒ è¨“ç·´å¤±æ•—")
            return None
            
    except Exception as e:
        print(f"âŒ è¨“ç·´éç¨‹ä¸­å‡ºç¾éŒ¯èª¤: {e}")
        return None

def train_chat_lora(epochs=3, max_samples=1000):
    """å°ˆé–€è¨“ç·´èŠå¤©LoRAæ¨¡å‹"""
    print("ğŸ’¬ é–‹å§‹è¨“ç·´èŠå¤©å°ˆç”¨LoRAæ¨¡å‹ (4bité‡åŒ–)")
    print("="*60)
    
    trainer = RushiaLLMTrainer(data_category="chat")
    
    try:
        # è¼‰å…¥æ¨¡å‹ï¼ˆ4bité‡åŒ–ï¼‰
        trainer.load_model_and_tokenizer()
        
        # è¨­ç½®LoRA
        trainer.setup_lora()
        
        # é–‹å§‹è¨“ç·´
        model_path = trainer.train(epochs=epochs, max_samples=max_samples)
        
        if model_path:
            print(f"\nğŸ‰ èŠå¤©LoRAæ¨¡å‹è¨“ç·´å®Œæˆï¼")
            print(f"ğŸ“ æ¨¡å‹ä¿å­˜åœ¨: {model_path}")
            return model_path
        else:
            print("âŒ è¨“ç·´å¤±æ•—")
            return None
            
    except Exception as e:
        print(f"âŒ è¨“ç·´éç¨‹ä¸­å‡ºç¾éŒ¯èª¤: {e}")
        return None

def train_all_available_categories():
    """è¨“ç·´æ‰€æœ‰å¯ç”¨çš„åˆ†é¡æ•¸æ“š - 4bité‡åŒ–ç‰ˆæœ¬"""
    # æª¢æŸ¥å¯ç”¨çš„åˆ†é¡æ•¸æ“šæ–‡ä»¶
    category_files = {
        "asmr": "D:/RushiaMode/training_data/by_category/rushia_ASMR_training.jsonl",
        "chat": "D:/RushiaMode/training_data/by_category/rushia_é›œè«‡_training.jsonl",
        "mixed": "D:/RushiaMode/training_data/rushia_mixed_training.jsonl"
    }
    
    available_categories = []
    for cat, file_path in category_files.items():
        if os.path.exists(file_path):
            available_categories.append(cat)
            print(f"âœ… ç™¼ç¾ {cat} è¨“ç·´æ•¸æ“š: {file_path}")
        else:
            print(f"âŒ æœªæ‰¾åˆ° {cat} è¨“ç·´æ•¸æ“š: {file_path}")
    
    if not available_categories:
        print("âŒ æ²’æœ‰æ‰¾åˆ°ä»»ä½•è¨“ç·´æ•¸æ“šæ–‡ä»¶ï¼")
        print("è«‹å…ˆé‹è¡Œ prepare_training_data_enhanced.py ç”Ÿæˆè¨“ç·´æ•¸æ“š")
        return
    
    print(f"\nç™¼ç¾ {len(available_categories)} å€‹å¯ç”¨é¡åˆ¥: {', '.join(available_categories)}")
    
    # è¨“ç·´æ¯å€‹å¯ç”¨é¡åˆ¥
    trained_models = []
    for category in available_categories:
        print(f"\n{'='*60}")
        print(f"é–‹å§‹è¨“ç·´: {category.upper()}")
        print(f"{'='*60}")
        
        try:
            if category == "asmr":
                model_path = train_asmr_lora(epochs=2, max_samples=800)
            elif category == "chat":
                model_path = train_chat_lora(epochs=2, max_samples=800)
            else:  # mixed
                trainer = RushiaLLMTrainer(data_category="all")
                trainer.load_model_and_tokenizer()
                trainer.setup_lora()
                model_path = trainer.train(epochs=2, max_samples=800)
            
            if model_path:
                trained_models.append((category, model_path))
                print(f"âœ… {category} è¨“ç·´å®Œæˆ")
            else:
                print(f"âŒ {category} è¨“ç·´å¤±æ•—")
                
        except Exception as e:
            print(f"âŒ {category} è¨“ç·´éç¨‹ä¸­å‡ºç¾éŒ¯èª¤: {e}")
    
    print(f"\nğŸ‰ å…¨éƒ¨è¨“ç·´å®Œæˆï¼å…±è¨“ç·´äº† {len(trained_models)} å€‹æ¨¡å‹:")
    for cat, path in trained_models:
        print(f"  ğŸ“ {cat}: {path}")

def train_all_available_categories_enhanced(enable_augmentation=True):
    """è¨“ç·´æ‰€æœ‰å¯ç”¨çš„åˆ†é¡æ•¸æ“š - æ”¯æ´è³‡æ–™å¢å¼·ç‰ˆæœ¬"""
    # æª¢æŸ¥å¯ç”¨çš„åˆ†é¡æ•¸æ“šæ–‡ä»¶
    category_files = {
        "asmr": "D:/RushiaMode/training_data/by_category/rushia_ASMR_training.jsonl",
        "chat": "D:/RushiaMode/training_data/by_category/rushia_é›œè«‡_training.jsonl",
        "mixed": "D:/RushiaMode/training_data/rushia_mixed_training.jsonl"
    }
    
    available_categories = []
    for cat, file_path in category_files.items():
        if os.path.exists(file_path):
            available_categories.append(cat)
            print(f"âœ… ç™¼ç¾ {cat} è¨“ç·´æ•¸æ“š: {file_path}")
        else:
            print(f"âŒ æœªæ‰¾åˆ° {cat} è¨“ç·´æ•¸æ“š: {file_path}")
    
    if not available_categories:
        print("âŒ æ²’æœ‰æ‰¾åˆ°ä»»ä½•è¨“ç·´æ•¸æ“šæ–‡ä»¶ï¼")
        print("è«‹å…ˆé‹è¡Œ prepare_training_data_enhanced.py ç”Ÿæˆè¨“ç·´æ•¸æ“š")
        return
    
    print(f"\nç™¼ç¾ {len(available_categories)} å€‹å¯ç”¨é¡åˆ¥: {', '.join(available_categories)}")
    print(f"è³‡æ–™å¢å¼·: {'å•Ÿç”¨' if enable_augmentation else 'é—œé–‰'}")
    
    # è¨“ç·´æ¯å€‹å¯ç”¨é¡åˆ¥
    trained_models = []
    for category in available_categories:
        print(f"\n{'='*60}")
        print(f"é–‹å§‹è¨“ç·´: {category.upper()}")
        print(f"{'='*60}")
        
        try:
            trainer = RushiaLLMTrainer(
                data_category=category if category != "mixed" else "all",
                enable_data_augmentation=enable_augmentation
            )
            trainer.load_model_and_tokenizer()
            trainer.setup_lora()
            model_path = trainer.train(epochs=2, max_samples=800, max_seq_length=384)
            
            if model_path:
                trained_models.append((category, model_path))
                print(f"âœ… {category} è¨“ç·´å®Œæˆ")
            else:
                print(f"âŒ {category} è¨“ç·´å¤±æ•—")
                
        except Exception as e:
            print(f"âŒ {category} è¨“ç·´éç¨‹ä¸­å‡ºç¾éŒ¯èª¤: {e}")
    
    print(f"\nğŸ‰ å…¨éƒ¨è¨“ç·´å®Œæˆï¼å…±è¨“ç·´äº† {len(trained_models)} å€‹æ¨¡å‹:")
    for cat, path in trained_models:
        print(f"  ğŸ“ {cat}: {path}")

def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸ¤– æ½¤ç¾½éœ²è¥¿äº LoRA è¨“ç·´ç³»çµ± (Windows å…¼å®¹ç‰ˆ)")
    print("="*70)
    
    # Windows å°ˆç”¨å„ªåŒ–
    optimize_for_rtx5070()
    print()
    
    # å„ªåŒ–è¨˜æ†¶é«”è¨­ç½®
    print("ğŸ”§ å„ªåŒ–è¨˜æ†¶é«”è¨­ç½®...")
    optimize_memory_settings()
    print()
    
    # æª¢æŸ¥GPUé¡¯å­˜
    print("æª¢æŸ¥ç³»çµ±è³‡æº...")
    gpu_memory = check_gpu_memory()
    print()
    
    if gpu_memory < 8:
        print("âš ï¸  è­¦å‘Š: é¡¯å­˜ä¸è¶³8GBï¼Œå°‡ä½¿ç”¨ä¿å®ˆçš„è¨“ç·´åƒæ•¸")
        print()
    
    print("é¸æ“‡è¨“ç·´æ¨¡å¼:")
    print("1. ğŸ§ ASMRå°ˆç”¨LoRAè¨“ç·´ (æ¨è–¦)")
    print("2. ğŸ’¬ èŠå¤©å°ˆç”¨LoRAè¨“ç·´")
    print("3. ğŸš€ è‡ªå‹•è¨“ç·´æ‰€æœ‰å¯ç”¨åˆ†é¡")
    print("4. ğŸ› ï¸ è‡ªå®šç¾©è¨“ç·´")
    
    choice = input("\nè«‹é¸æ“‡ (1/2/3/4): ").strip()
    
    # è©¢å•æ˜¯å¦å•Ÿç”¨è³‡æ–™å¢å¼·
    augment_choice = input("\næ˜¯å¦å•Ÿç”¨è³‡æ–™å¢å¼·ï¼Ÿ(Y/n): ").strip().lower()
    enable_augmentation = augment_choice != 'n'
    
    if choice == "1":
        epochs = int(input("è«‹è¼¸å…¥è¨“ç·´è¼ªæ•¸ (å»ºè­°2-3): ") or "3")
        max_samples = int(input("è«‹è¼¸å…¥æœ€å¤§æ¨£æœ¬æ•¸ (å»ºè­°500-1000): ") or "800")
        max_seq_length = int(input("è«‹è¼¸å…¥è¨“ç·´æ™‚æœ€å¤§åºåˆ—é•·åº¦ (å»ºè­°256-512): ") or "384")
        
        trainer = RushiaLLMTrainer(data_category="asmr", enable_data_augmentation=enable_augmentation)
        trainer.load_model_and_tokenizer()
        trainer.setup_lora()
        trainer.train(epochs=epochs, max_samples=max_samples, max_seq_length=max_seq_length)
        
    elif choice == "2":
        epochs = int(input("è«‹è¼¸å…¥è¨“ç·´è¼ªæ•¸ (å»ºè­°2-3): ") or "3")
        max_samples = int(input("è«‹è¼¸å…¥æœ€å¤§æ¨£æœ¬æ•¸ (å»ºè­°500-1000): ") or "800")
        max_seq_length = int(input("è«‹è¼¸å…¥è¨“ç·´æ™‚æœ€å¤§åºåˆ—é•·åº¦ (å»ºè­°256-512): ") or "384")
        
        trainer = RushiaLLMTrainer(data_category="chat", enable_data_augmentation=enable_augmentation)
        trainer.load_model_and_tokenizer()
        trainer.setup_lora()
        trainer.train(epochs=epochs, max_samples=max_samples, max_seq_length=max_seq_length)
        
    elif choice == "3":
        # è‡ªå‹•è¨“ç·´æ‰€æœ‰åˆ†é¡æ™‚ï¼Œé»˜èªå•Ÿç”¨å¢å¼·
        print("ğŸš€ è‡ªå‹•è¨“ç·´æ¨¡å¼ï¼Œå•Ÿç”¨æ‰€æœ‰å„ªåŒ–åŠŸèƒ½")
        train_all_available_categories_enhanced(enable_augmentation)
        
    elif choice == "4":
        print("\nå¯ç”¨åˆ†é¡:")
        print("- asmr: ASMRå°ˆç”¨æ¨¡å‹")
        print("- chat: æ—¥å¸¸èŠå¤©æ¨¡å‹")
        print("- all: æ··åˆæ•¸æ“šæ¨¡å‹")
        
        category = input("è«‹é¸æ“‡åˆ†é¡ (asmr/chat/all): ").strip().lower()
        if category in ["asmr", "chat", "all"]:
            epochs = int(input("è«‹è¼¸å…¥è¨“ç·´è¼ªæ•¸ (å»ºè­°2-5): ") or "3")
            max_samples = int(input("è«‹è¼¸å…¥æœ€å¤§æ¨£æœ¬æ•¸ (å»ºè­°500-1500): ") or "1000")
            max_seq_length = int(input("è«‹è¼¸å…¥è¨“ç·´æ™‚æœ€å¤§åºåˆ—é•·åº¦ (å»ºè­°256-512): ") or "384")
            
            trainer = RushiaLLMTrainer(data_category=category, enable_data_augmentation=enable_augmentation)
            trainer.load_model_and_tokenizer()
            trainer.setup_lora()
            trainer.train(epochs=epochs, max_samples=max_samples, max_seq_length=max_seq_length)
        else:
            print("ç„¡æ•ˆçš„åˆ†é¡é¸æ“‡")
            
    else:
        print("ç„¡æ•ˆé¸æ“‡")

if __name__ == "__main__":
    main()
